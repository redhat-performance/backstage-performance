- name: measurements.cluster_cpu_usage_seconds_total_rate
  monitoring_query: sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{cluster=""})
  monitoring_step: 15

- name: measurements.cluster_memory_usage_rss_total
  monitoring_query: sum(container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", cluster="", container!=""})
  monitoring_step: 15

- name: measurements.cluster_disk_throughput_total
  monitoring_query: sum (rate(container_fs_reads_bytes_total{id!="", device=~"(/dev.+)|mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+", cluster=""}[5m]) + rate(container_fs_writes_bytes_total{id!="", device=~"(/dev.+)|mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+", cluster=""}[5m]))
  monitoring_step: 15

- name: measurements.cluster_network_bytes_total
  monitoring_query: sum(irate(container_network_receive_bytes_total{cluster="",namespace=~".*"}[5m])) + sum(irate(container_network_transmit_bytes_total{cluster="",namespace=~".*"}[5m]))
  monitoring_step: 15

- name: measurements.cluster_network_receive_bytes_total
  monitoring_query: sum(irate(container_network_receive_bytes_total{cluster="",namespace=~".*"}[5m]))
  monitoring_step: 15

- name: measurements.cluster_network_transmit_bytes_total
  monitoring_query: sum(irate(container_network_transmit_bytes_total{cluster="",namespace=~".*"}[5m]))
  monitoring_step: 15

- name: measurements.node_disk_io_time_seconds_total
  monitoring_query: sum(irate(node_disk_io_time_seconds_total{cluster="",namespace=~".*"}[5m]))
  monitoring_step: 15

- name: measurements.cluster_nodes_worker_count
  monitoring_query: count(kube_node_role{role="worker"})
  monitoring_step: 15

- name: measurements.cluster_pods_count
  monitoring_query: count(kube_pod_info)
  monitoring_step: 15

- name: measurements.cluster_running_pods_on_workers_count
  monitoring_query: count(kube_pod_info * on(node) group_left(role) kube_node_role{role="worker"} and on(pod, namespace) (kube_pod_status_phase{job="kube-state-metrics", phase="Running"} > 0))
  monitoring_step: 15

- name: measurements.etcd_request_duration_seconds_average
  monitoring_query: sum(rate(etcd_request_duration_seconds_sum{}[5m])) / sum(rate(etcd_request_duration_seconds_count[5m]))
  monitoring_step: 15



# Interesting CI environment variables
{% for var in [
  'BUILD_ID',
  'HOSTNAME',
  'JOB_NAME',
  'OPENSHIFT_API',
  'PROW_JOB_ID',
  'PULL_BASE_REF',
  'PULL_BASE_SHA',
  'PULL_HEAD_REF',
  'PULL_NUMBER',
  'PULL_PULL_SHA',
  'PULL_REFS',
  'REPO_NAME',
  'REPO_OWNER',
  'USERS',
  'WORKERS',
  'DURATION',
  'SPAWN_RATE',
  'SCENARIO',
  'PRE_LOAD_DB',
  'RHDH_DEPLOYMENT_REPLICAS',
  'RHDH_DB_REPLICAS',
  'RHDH_DB_STORAGE',
  'RHDH_RESOURCES_CPU_REQUESTS',
  'RHDH_RESOURCES_CPU_LIMITS',
  'RHDH_RESOURCES_MEMORY_REQUESTS',
  'RHDH_RESOURCES_MEMORY_LIMITS',
  'RHDH_KEYCLOAK_REPLICAS',
  'RHDH_HELM_REPO',
  'RHDH_HELM_CHART',
  'RHDH_HELM_CHART_VERSION',
  'RHDH_HELM_RELEASE_NAME',
  'RHDH_IMAGE_REGISTRY',
  'RHDH_IMAGE_REPO',
  'RHDH_IMAGE_TAG',
  'API_COUNT',
  'COMPONENT_COUNT',
  'BACKSTAGE_USER_COUNT',
  'GROUP_COUNT',
  'WAIT_FOR_SEARCH_INDEX',
  'SCALE_WORKERS',
  'SCALE_ACTIVE_USERS_SPAWN_RATES',
  'SCALE_BS_USERS_GROUPS',
  'SCALE_CATALOG_SIZES',
  'SCALE_REPLICAS',
  'SCALE_DB_STORAGES'
] %}
- name: metadata.env.{{ var }}
  env_variable: {{ var }}
{% endfor %}

- name: metadata.git.last_commit.hash
  command: git log -1 --pretty=format:"%H"



# Gather some basic info about the cluster
- name: metadata.cluster.context
  command: oc project default > /dev/null && oc config current-context

- name: metadata.cluster.control-plane.count
  command: oc get nodes -l node-role.kubernetes.io/master -o name | wc -l

- name: metadata.cluster.control-plane.flavor
  command: oc get nodes -l node-role.kubernetes.io/master -o json | jq --raw-output '.items | map(.metadata.labels."beta.kubernetes.io/instance-type") | unique | sort | join(",")'

- name: metadata.cluster.control-plane.nodes
  command: oc get nodes -l node-role.kubernetes.io/master -o json | jq '.items | map(.metadata.name)'
  output: json

- name: metadata.cluster.compute-nodes.count
  command: oc get nodes -l node-role.kubernetes.io/worker -o name | wc -l

- name: metadata.cluster.compute-nodes.flavor
  command: oc get nodes -l node-role.kubernetes.io/worker -o json | jq --raw-output '.items | map(.metadata.labels."beta.kubernetes.io/instance-type") | unique | sort | join(",")'

- name: metadata.cluster.compute-nodes.nodes
  command: oc get nodes -l node-role.kubernetes.io/worker -o json | jq '.items | map(.metadata.name)'
  output: json



{% macro monitor_pod(alias, namespace_regex, pod_regex, step=15, pod_suffix_regex='-[0-9a-f]+-.*') -%}
# Gather monitoring data about the pod
- name: measurements.{{ alias }}.cpu
  monitoring_query: sum(pod:container_cpu_usage:sum{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}'})
  monitoring_step: {{ step }}
- name: measurements.{{ alias }}.memory
  monitoring_query: sum(container_memory_usage_bytes{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}', container!='POD', container!=''})
  monitoring_step: {{ step }}
- name: measurements.{{ alias }}.network_throughput
  monitoring_query: sum( rate(container_network_transmit_bytes_total{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}'}[{{ step * 4 }}s]) + rate(container_network_receive_bytes_total{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}'}[{{ step * 4 }}s]) )
  monitoring_step: {{ step * 4 }}
- name: measurements.{{ alias }}.network_drop
  monitoring_query: sum( rate(container_network_transmit_packets_dropped_total{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}'}[{{ step * 4 }}s]) + rate(container_network_receive_packets_dropped_total{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}'}[{{ step * 4 }}s]) )
  monitoring_step: {{ step * 4 }}
- name: measurements.{{ alias }}.disk_throughput
  monitoring_query: sum( sum(rate(container_fs_reads_bytes_total{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}', device!='/dev/dm-0'}[{{ step * 4 }}s])) + sum(rate(container_fs_writes_bytes_total{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}', device!='/dev/dm-0'}[{{ step * 4 }}s])) )
  monitoring_step: {{ step * 4 }}
- name: measurements.{{ alias }}.restarts
  monitoring_query: sum(kube_pod_container_status_restarts_total{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}'})
  monitoring_step: {{ step }}
- name: measurements.{{ alias }}.count_ready
  monitoring_query: sum( kube_pod_status_ready{namespace=~'{{ namespace_regex }}', pod=~'{{ pod_regex }}{{ pod_suffix_regex }}'} )
  monitoring_step: {{ step }}
{%- endmacro %}

{% macro pod_info(alias, namespace_regex, deployment_regex, container) -%}
# Gather info about pod configuration
- name: metadata.cluster.pods.{{ alias }}.count
  command: oc get deployments -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ deployment_regex }}")).spec | if has("replicas") then .replicas else 1 end'
- name: metadata.cluster.pods.{{ alias }}.resources
  command: oc get deployments -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ deployment_regex }}")).spec.template.spec.containers | map(select(.name == "{{ container }}"))[0].resources'
  output: json
- name: metadata.cluster.pods.{{ alias }}.image
  command: oc get deployments -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ deployment_regex }}")).spec.template.spec.containers | map(select(.name == "{{ container }}"))[0].image'
- name: metadata.cluster.pods.{{ alias }}.image_tag
  command: oc get deployments -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ deployment_regex }}")).spec.template.spec.containers | map(select(.name == "{{ container }}"))[0].image | split(":")[1]'
{%- endmacro %}



# Collect data for relevant pods
{{ monitor_pod('rhdh-developer-hub', 'rhdh-performance.*', '(rhdh|backstage)-developer-hub', 15) }}
{{ monitor_pod('rhdh-postgresql', 'rhdh-performance.*', '(rhdh|backstage)-(postgresql|psql)', 15, '-.*') }}
{{ pod_info('rhdh-developer-hub-backstage-backend', 'rhdh-performance.*', '(rhdh|backstage)-developer-hub', 'backstage-backend') }}



# Collect data for API pods
{{ monitor_pod('apiserver', 'openshift-apiserver', 'apiserver', 15) }}
{{ monitor_pod('kube-apiserver', 'openshift-kube-apiserver', 'kube-apiserver', 15, pod_suffix_regex='-ip-.+') }}

{% macro pv_stats(alias, pvc_regex) -%}
# Collect data for PV stats
- name: measurements.cluster.pv_stats.test.{{alias}}.capacity_bytes
  monitoring_query: kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"{{ pvc_regex }}"}
  monitoring_step: 15
- name: measurements.cluster.pv_stats.test.{{alias}}.used_bytes
  monitoring_query: kubelet_volume_stats_used_bytes{persistentvolumeclaim=~"{{ pvc_regex }}"}
  monitoring_step: 15
- name: measurements.cluster.pv_stats.test.{{alias}}.available_bytes
  monitoring_query: kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"{{ pvc_regex }}"}
  monitoring_step: 15
{%- endmacro %}

{{ pv_stats('rhdh-postgresql', 'data-(rhdh|backstage)-(postgresql|psql)-(primary|developer-hub)-0') }}

# Collect index usage
#Note: It is assumed that the default value forÂ namespace and pod name is used.
{% macro collect_index_usage(namespace_regex, pod_regex) -%}
- name: measurements.postgresql.backstage-plugin-catalog.index
  command: oc exec $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.name') -n $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.namespace') -- psql -h localhost -U postgres backstage_plugin_catalog -c "SELECT relname, 100 * idx_scan / (seq_scan + idx_scan) percent_of_times_index_used, n_live_tup rows_in_table FROM pg_stat_user_tables ORDER BY n_live_tup DESC;" -A -F ',' |head -n -1|yq -p csv -o json
  output: json

- name: measurements.postgresql.backstage-plugin-auth.index
  command: oc exec $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.name') -n $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.namespace') -- psql -h localhost -U postgres backstage_plugin_auth -c "SELECT relname, 100 * idx_scan / (seq_scan + idx_scan) percent_of_times_index_used, n_live_tup rows_in_table FROM pg_stat_user_tables ORDER BY n_live_tup DESC;" -A -F ',' |head -n -1|yq -p csv -o json
  output: json

- name: measurements.postgresql.backstage-plugin-app.index
  command: oc exec $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.name') -n $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.namespace') -- psql -h localhost -U postgres backstage_plugin_app -c "SELECT relname, 100 * idx_scan / (seq_scan + idx_scan) percent_of_times_index_used, n_live_tup rows_in_table FROM pg_stat_user_tables ORDER BY n_live_tup DESC;" -A -F ',' |head -n -1|yq -p csv -o json
  output: json

- name: measurements.postgresql.backstage-plugin-scaffolder.index
  command: oc exec $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.name') -n $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.namespace') -- psql -h localhost -U postgres backstage_plugin_scaffolder -c "SELECT relname, 100 * idx_scan / (seq_scan + idx_scan) percent_of_times_index_used, n_live_tup rows_in_table FROM pg_stat_user_tables ORDER BY n_live_tup DESC;" -A -F ',' |head -n -1|yq -p csv -o json
  output: json

- name: measurements.postgresql.backstage-plugin-search.index
  command: oc exec $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.name') -n $(oc get pods -A -o json | jq -r '.items[] | select(.metadata.namespace | match("{{ namespace_regex }}")) | select(.metadata.name | match("{{ pod_regex }}")).metadata.namespace') -- psql -h localhost -U postgres backstage_plugin_search -c "SELECT relname, 100 * idx_scan / (seq_scan + idx_scan) percent_of_times_index_used, n_live_tup rows_in_table FROM pg_stat_user_tables ORDER BY n_live_tup DESC;" -A -F ',' |head -n -1|yq -p csv -o json
  output: json
{%- endmacro %}

{{ collect_index_usage('rhdh-performance.*', '(rhdh|backstage)-(postgresql|psql)-(primary|developer-hub)-0') }}


{% macro rhdh_nodejs_lst( query, label, valuelst) -%}
{% for value  in valuelst %}
# Gather nodejs monitoring data about the {{ query }}
- name: measurements.nodejs.test.{{ query }}.{{ label }}.{{ 
value }}
  monitoring_query: sum({{ query }}{ {{ label }}="{{ value }}", job="rhdh-metrics" })
  monitoring_step: 15
{% endfor %}
{%- endmacro %}

{{ rhdh_nodejs_lst('catalog_processors_duration_seconds_sum', 'result', ['ok','failed']) }}
{{ rhdh_nodejs_lst('catalog_processors_duration_seconds_count', 'result', ['ok','failed']) }}
{{ rhdh_nodejs_lst('catalog_processing_duration_seconds_sum', 'result', ['unchanged']) }}
{{ rhdh_nodejs_lst('catalog_processing_duration_seconds_count', 'result', ['unchanged']) }}
{{ rhdh_nodejs_lst('nodejs_gc_duration_seconds_sum', 'kind', ['minor','major','incremental']) }}
{{ rhdh_nodejs_lst('nodejs_gc_duration_seconds_count', 'kind', ['minor','major','incremental']) }}
{{ rhdh_nodejs_lst('catalog_entities_count', 'kind', ['location','user','group']) }}

{% macro rhdh_nodejs_rate( query ) -%}
# Gather nodejs monitoring data about the {{ query }}
- name: measurements.nodejs.test.{{ query }}
  monitoring_query: sum(rate({{ query }}{ job="rhdh-metrics" }[5m]))
  monitoring_step: 15
{%- endmacro %}

{% for query in [
'process_cpu_user_seconds_total',
'process_cpu_system_seconds_total',
'process_cpu_seconds_total',
'catalog_stitched_entities_count'
] %}
{{ rhdh_nodejs_rate(query) }}
{% endfor %}

{% macro rhdh_nodejs( query ) -%}
# Gather nodejs  monitoring data about the {{ query }}
- name: measurements.nodejs.test.{{ query }}
  monitoring_query: sum({{ query }}{ job="rhdh-metrics" })
  monitoring_step: 15
{%- endmacro %}

{% for query in [
'process_resident_memory_bytes',
'process_virtual_memory_bytes',
'process_heap_bytes',
'process_open_fds',
'nodejs_eventloop_lag_seconds',
'nodejs_eventloop_lag_mean_seconds',
'nodejs_eventloop_lag_stddev_seconds',
'nodejs_eventloop_lag_p90_seconds',
'nodejs_active_resources_total',
'nodejs_active_handles_total',
'nodejs_active_requests_total',
'nodejs_heap_size_total_bytes',
'nodejs_heap_size_used_bytes',
'nodejs_external_memory_bytes',
'catalog_registered_locations_count',
'catalog_relations_count',
'catalog_processing_queue_delay_seconds_sum',
'catalog_processing_queue_delay_seconds_count'
] %}
{{ rhdh_nodejs(query) }}
{% endfor %}

- name: measurements.nodejs.test.catalog_processing_queue_delay_seconds_average
  monitoring_query: sum(rate(catalog_processing_queue_delay_seconds_sum{job="rhdh-metrics"}[5m]))/sum(rate(catalog_processing_queue_delay_seconds_count{job="rhdh-metrics"}[5m]))
  monitoring_step: 15

- name: measurements.nodejs.test.catalog_processors_duration_seconds_failed_average
  monitoring_query: sum(rate(catalog_processors_duration_seconds_sum{result="failed",job="rhdh-metrics"}[5m]))/sum(rate(catalog_processors_duration_seconds_count{result="failed",job="rhdh-metrics"}[5m]))
  monitoring_step: 15

- name: measurements.nodejs.test.nodejs_gc_duration_seconds_major_average
  monitoring_query: sum(rate(nodejs_gc_duration_seconds_sum{kind="major",job="rhdh-metrics"}[5m]))/sum(rate(nodejs_gc_duration_seconds_count{kind="major",job="rhdh-metrics"}[5m]))
  monitoring_step: 15


{% macro pg_query_sum(alias, query) -%}
# Gather monitoring data about the db {{ alias }}
- name: measurements.postgresql.test.{{ alias }}.{{ query }}
  monitoring_query: sum({{ query }}{service='{{ alias }}-prometheus-postgres-exporter'})
  monitoring_step: 15
{%- endmacro %}

{% for query in [
'pg_statio_user_indexes_idx_blks_hit_total',
'pg_statio_user_indexes_idx_blks_read_total',
'pg_statio_user_tables_heap_blocks_hit',
'pg_statio_user_tables_heap_blocks_read',
'pg_statio_user_tables_idx_blocks_hit',
'pg_statio_user_tables_idx_blocks_read',
'pg_statio_user_tables_tidx_blocks_hit',
'pg_statio_user_tables_tidx_blocks_read',
'pg_statio_user_tables_toast_blocks_hit',
'pg_statio_user_tables_toast_blocks_read',
'pg_stat_user_tables_vacuum_count',
'pg_stat_user_tables_size_bytes',
'pg_stat_user_tables_seq_tup_read',
'pg_stat_user_tables_seq_scan',
'pg_stat_user_tables_n_tup_upd',
'pg_stat_user_tables_n_tup_ins',
'pg_stat_user_tables_n_tup_hot_upd',
'pg_stat_user_tables_n_tup_del',
'pg_stat_user_tables_n_mod_since_analyze',
'pg_stat_user_tables_n_live_tup',
'pg_stat_user_tables_n_dead_tup',
'pg_stat_user_tables_last_vacuum',
'pg_stat_user_tables_last_autovacuum',
'pg_stat_user_tables_last_autoanalyze',
'pg_stat_user_tables_last_analyze',
'pg_stat_user_tables_idx_tup_fetch',
'pg_stat_user_tables_idx_scan',
'pg_stat_user_tables_autovacuum_count',
'pg_stat_user_tables_autoanalyze_count',
'pg_stat_user_tables_analyze_count'
] %}
{% for db in [
'backstage-plugin-permission',
'backstage-plugin-auth',
'backstage-plugin-catalog',
'backstage-plugin-scaffolder',
'backstage-plugin-search',
'backstage-plugin-app'
] %}
{{ pg_query_sum(db, query ) }}
{% endfor %}
{% endfor %}


{% macro pg_query(alias, query) -%}
# Gather monitoring data about the db {{ alias }}
- name: measurements.postgresql.test.{{ alias }}.{{ query }}
  monitoring_query: {{ query }}{datname="{{ alias }}"}
  monitoring_step: 15
{%- endmacro %}

{% for query in [
'pg_stat_database_blk_read_time',
'pg_stat_database_blk_write_time',
'pg_stat_database_blks_hit',
'pg_stat_database_blks_read',
'pg_stat_database_conflicts',
'pg_stat_database_conflicts_confl_bufferpin',
'pg_stat_database_conflicts_confl_deadlock',
'pg_stat_database_conflicts_confl_lock',
'pg_stat_database_conflicts_confl_snapshot',
'pg_stat_database_conflicts_confl_tablespace',
'pg_stat_database_deadlocks',
'pg_stat_database_numbackends',
'pg_stat_database_temp_bytes',
'pg_stat_database_temp_files',
'pg_stat_database_tup_deleted',
'pg_stat_database_tup_fetched',
'pg_stat_database_tup_inserted',
'pg_stat_database_tup_returned',
'pg_stat_database_tup_updated',
'pg_stat_database_xact_commit',
'pg_stat_database_xact_rollback',
'pg_database_size_bytes'
] %}
{% for db in [
'backstage_plugin_permission',
'backstage_plugin_auth',
'backstage_plugin_catalog',
'backstage_plugin_scaffolder',
'backstage_plugin_search',
'backstage_plugin_app'
] %}
{{ pg_query(db, query ) }}
{% endfor %}
{% endfor %}

{% macro pg_stat_statements_sum(alias, query) -%}
# Gather monitoring data about the db {{ alias }}
- name: measurements.postgresql.test.{{ alias }}.{{ query }}
  monitoring_query: sum({{ query }}{datname='{{ alias }}'})
  monitoring_step: 15
{%- endmacro %}

{% for query in [
'pg_stat_statements_block_read_seconds_total',
'pg_stat_statements_block_write_seconds_total',
'pg_stat_statements_calls_total',
'pg_stat_statements_rows_total',
'pg_stat_statements_seconds_total',
'pg_locks_count'
] %}
{% for db in [
'backstage_plugin_permission',
'backstage_plugin_auth',
'backstage_plugin_catalog',
'backstage_plugin_scaffolder',
'backstage_plugin_search',
'backstage_plugin_app'
] %}
{{ pg_stat_statements_sum(db, query ) }}
{% endfor %}
{% endfor %}


{% macro pg_settings(query) -%}
# Gather monitoring data about the db {{ alias }}
- name: measurements.postgresql.test.{{ query }}
  monitoring_query: {{ query }}{service="pg-exporter-prometheus-postgres-exporter"}
  monitoring_step: 30
{%- endmacro %}

{% for query in [
'pg_settings_max_connections',
'pg_settings_superuser_reserved_connections',
'pg_settings_shared_buffers_bytes',
'pg_settings_work_mem_bytes',
'pg_settings_maintenance_work_mem_bytes',
'pg_settings_shared_memory_size_in_huge_pages',
'pg_settings_effective_cache_size_bytes',
'pg_settings_effective_io_concurrency',
'pg_settings_random_page_cost',
'pg_settings_track_io_timing',
'pg_settings_max_wal_senders',
'pg_settings_checkpoint_timeout_seconds',
'pg_settings_checkpoint_completion_target',
'pg_settings_max_wal_size_bytes',
'pg_settings_min_wal_size_bytes',
'pg_settings_wal_buffers_bytes',
'pg_settings_wal_writer_delay_seconds',
'pg_settings_wal_writer_flush_after_bytes',
'pg_settings_bgwriter_delay_seconds',
'pg_settings_bgwriter_lru_maxpages',
'pg_settings_bgwriter_lru_multiplier',
'pg_settings_bgwriter_flush_after_bytes',
'pg_settings_max_worker_processes',
'pg_settings_max_parallel_workers_per_gather',
'pg_settings_max_parallel_maintenance_workers',
'pg_settings_max_parallel_workers',
'pg_settings_parallel_leader_participation',
'pg_settings_enable_partitionwise_join',
'pg_settings_enable_partitionwise_aggregate',
'pg_settings_jit',
'pg_settings_max_slot_wal_keep_size_bytes',
'pg_settings_track_wal_io_timing',
'pg_settings_maintenance_io_concurrency',
'pg_settings_wal_recycle',
'pg_process_idle_seconds_sum',
'pg_process_idle_seconds_count',
'pg_stat_bgwriter_buffers_alloc_total',
'pg_stat_bgwriter_buffers_backend_fsync_total',
'pg_stat_bgwriter_buffers_backend_total',
'pg_stat_bgwriter_buffers_checkpoint_tota',
'pg_stat_bgwriter_buffers_clean_total',
'pg_stat_bgwriter_checkpoint_sync_time_total',
'pg_stat_bgwriter_checkpoint_write_time_total',
'pg_stat_bgwriter_checkpoints_req_total',
'pg_stat_bgwriter_checkpoints_timed_total',
'pg_stat_bgwriter_maxwritten_clean_total',
'pg_stat_archiver_archived_count',
'pg_stat_archiver_failed_count',
'pg_long_running_transactions',
'pg_long_running_transactions_oldest_timestamp_seconds',
'pg_wal_segments',
'pg_wal_size_bytes',
'process_cpu_seconds_total',
'process_max_fds',
'process_open_fds',
'process_resident_memory_bytes',
'process_virtual_memory_bytes',
'process_virtual_memory_max_bytes',
] %}
{{ pg_settings( query ) }}
{% endfor %}

{% macro pg_stat_activity(alias, query, state) -%}
# Gather monitoring data about the db {{ alias }}
- name: measurements.postgresql.test.{{ alias }}.{{ query }}.{{ state }}
  monitoring_query: sum({{ query }}{datname='{{ alias }}',state='{{ state }}',service="pg-exporter-prometheus-postgres-exporter"})
  monitoring_step: 15
{%- endmacro %}

{% for query in [
'pg_stat_activity_count',
'pg_stat_activity_max_tx_duration'
] %}
{% for db in [
'backstage_plugin_permission',
'backstage_plugin_auth',
'backstage_plugin_catalog',
'backstage_plugin_scaffolder',
'backstage_plugin_search',
'backstage_plugin_app'
] %}
{% for state in [
'active',
'disabled',
'fastpath',
'idle'
] %}
{{ pg_stat_activity(db, query, state ) }}
{% endfor %}
{% endfor %}
{% endfor %}

# Results
{%macro results_scenario(name) -%}
- name: results.{{name}}.locust_requests_avg_response_time
  monitoring_query: sum(locust_requests_avg_response_time{name="{{name}}"})
  monitoring_step: 15
- name: results.{{name}}.locust_requests_avg_content_length
  monitoring_query: sum(locust_requests_avg_content_length{name="{{name}}"})
  monitoring_step: 15
- name: results.{{name}}.locust_requests_current_rps
  monitoring_query: sum(locust_requests_current_rps{name="{{name}}"})
  monitoring_step: 15
- name: results.{{name}}.locust_requests_current_fail_per_sec
  monitoring_query: sum(locust_requests_current_fail_per_sec{name="{{name}}"})
  monitoring_step: 15
- name: results.{{name}}.locust_requests_num_failures
  monitoring_query: sum(locust_requests_num_failures{name="{{name}}"})
  monitoring_step: 15
- name: results.{{name}}.locust_errors
  monitoring_query: sum(locust_errors{name="{{name}}"})
  monitoring_step: 15
{%- endmacro %}

- name: results.locust_requests_fail_ratio
  monitoring_query: locust_requests_fail_ratio
  monitoring_step: 15
- name: results.locust_users
  monitoring_query: locust_users
  monitoring_step: 15

{{ results_scenario('Aggregated') }}
